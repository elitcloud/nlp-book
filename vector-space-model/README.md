# Vector Space Model

A **vector space model** \(VSM\) represents input text into a vector space so that the input can be processed by machine learning algorithms.  Traditional NLP approaches adapted VSMs using sparse vectors that involved techniques such as [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) or [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing).  However, modern NLP approaches adapt VSMs using dense vectors generated by techniques such as [Word2Vec](word2vec.md), [FastText](fasttext.md), or [GloVe](glove.md), which learn word embeddings from large data using contents around individual words.  Since most state-of-the-art results in NLP are achieved by the modern approaches, this chapter focuses on explaining how word embeddings are generated and supported by ELIT.

## VectorSpaceModel

```python
class VectorSpaceModel(abc.ABC):
    def __init__(self, model, dim: int):
        self.model = model
        self.dim = dim

        # for zero padding
        self.pad = np.zeros(dim).astype('float32')

    @abc.abstractmethod
    def _emb(self, key: str) -> numpy.array:
        return

    def embedding(self, key: str=None) -> numpy.array:
        return self._emb(key) if key is not None else self.pad

    def embedding_list(self, keys: list of str) -> list of numpy.array:
        return [self.embedding(key) for key in keys]

    def document_matrix(self, tokens: list of str, maxlen: int) -> numpy.array:
        size = len(tokens)
        return [self.embedding(tokens[i]) if i < size else self.pad for i in range(maxlen)]
```

* `__init__`:
* `_emb`:
* `embedding`:
* `embedding_list`:
* `document_matrix`:

